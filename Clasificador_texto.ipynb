{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOjefS5KKOsErfVPsTmB/tV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanAcevedo08/DeepLearning/blob/main/Clasificador_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clasificador de texto en pytorch\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Pytorch ofrece soluciones a diferentes problemas como\n",
        "\n",
        "\n",
        "\n",
        "*   torchaudio: Nos ofrece poder manipular audio y usar para entrenar modelos\n",
        "*   torchtexto : Herramientas para manejar texto\n",
        "*   torchvision : Herramientas para manejar imagenes\n",
        "*   torchElastic : Cuando tenemos nuestros modelos deployeados estos nodos o maquinas que manejan cpus si una se cae o se agrega una puede caerse el modelo elastic se encarga de monitorear y controlarlas par aque no se caigan(entrenamiento distribuido y tolerante a fallos)\n",
        "*   torchServe: ayuda a deployear modelos en produccion\n",
        "\n"
      ],
      "metadata": {
        "id": "X6_m3OqQmSJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Instalar librerias y alinearlas"
      ],
      "metadata": {
        "id": "BMB9PaNFCWuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch==2.2.0 torchvision==0.17.2 torchaudio==2.2.0\n",
        "!pip install torchtext==0.17.2 torchdata==0.7.1 portalocker>=2.0.0"
      ],
      "metadata": {
        "id": "Qab29bvQ1hZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchtext, torchdata\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"torchtext:\", torchtext.__version__)\n",
        "print(\"torchdata:\", torchdata.__version__)"
      ],
      "metadata": {
        "id": "f08Rvige1q7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explorar rapidmente los datos"
      ],
      "metadata": {
        "id": "RR0k4CV34uV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import AG_NEWS #Importar dataset de texto a utilizar"
      ],
      "metadata": {
        "id": "502x_qzY58hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mostrar rapidamente el texto\n",
        "train_ite = iter(AG_NEWS(split=\"train\"))\n",
        "next(train_ite)"
      ],
      "metadata": {
        "id": "s9AdcNHJSppT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesar el texto"
      ],
      "metadata": {
        "id": "RRP4ZhXkCTAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Tokenizacion\n",
        "Vocab: id para cada token\n",
        "vocab to id\n",
        "embedding"
      ],
      "metadata": {
        "id": "9zHA3M_3CijJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer #Tokenizzador para el texto\n",
        "from torchtext.vocab import build_vocab_from_iterator #Consturctor de vocabulario\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\") #Tipo de tokenizador\n",
        "train_iter = AG_NEWS(split=\"train\") #Instanciar datos de entrenamiento"
      ],
      "metadata": {
        "id": "FpUFGjSFEV09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crear funcion que devuelve los toknes\n",
        "def yield_token(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text) #Yield para retornar el texto tokenizado\n",
        "\n",
        "#Crear vocabulario\n",
        "vocab = build_vocab_from_iterator(yield_token(train_iter), specials=[\"<unk>\"]) #Especal para cuando el modello tiene palabras que no sabe las marque como unknow\n",
        "#Establecer que cada que no encuentre un token en el vocabulario lo lleve a unknow\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "SAssnbugEg4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Probar elvocabulario con nuevas palabrass\n",
        "tst_text = \"Hello the day is kinda weird here in Colombia\"\n",
        "vocab(tokenizer(tst_text))"
      ],
      "metadata": {
        "id": "RV593ygqGrl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones automatizadoras"
      ],
      "metadata": {
        "id": "imItEyRP6hcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definir funciones para agilizar procesos\n",
        "#Funcion lambda para darle un id del vocabulario si lo encuntra\n",
        "text_to_id = lambda x: vocab(tokenizer(x))\n",
        "#Revisar que funcione\n",
        "print(text_to_id(tst_text))\n",
        "\n",
        "#Funcion para labels, como python empiezza desde 0 si yo pongo un label 10 que es un texto este me devuelve el texto 11 no el 10 entonces 10 tendría que ser 9 para qeu me devuelva el 10\n",
        "label_index = lambda n: int(n) - 1\n",
        "print(label_index(10)) # -> tiene que ser 9"
      ],
      "metadata": {
        "id": "A3niD6a6Ir2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcion collate>fn"
      ],
      "metadata": {
        "id": "Gzq0nw5b6mGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definir funcion collatefn\n",
        "#Empezar fijando el device, importante para operacione de tensores\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
        "\n",
        "#Definir la función\n",
        "def collate_batch(batch):\n",
        "  \"\"\"\n",
        "  El modelo divide la totalidad de los datos en batches , a lo que voy a\n",
        "  almacenar los datos en un solo tensor no en n batches diferentes como tensor para mi modelo\n",
        "\n",
        "  labels_batch: Todas las etiquetas de la cantidad de batches que tengamos en ese momento ejemplo [1], [2], [3],[4]  // [1,2,3,4]\n",
        "  tokens_batchs: Almacenará todos los tokens diferentes que ingresen en ese batch\n",
        "  offsets: Indica donde va inicar la palabra ya que vamos a tener un vector comoe esto [palabra, algo ......] y ahí en uno estpan todos entonces tengo que indicar donde empiza cada frase diferente\n",
        "\n",
        "  return: 3 Tensores torch en divice cuda o cpu para red neuronal\n",
        "  \"\"\"\n",
        "  labels_batch = []\n",
        "  tokens_batch = []\n",
        "  offsets = [0] #Donde va empezar la palabra\n",
        "\n",
        "  for (_label, _text) in batch:\n",
        "    labels_batch.append(label_index(_label)) #Para que se ajuste el numero con el dato real\n",
        "    process_text = torch.tensor(text_to_id(_text), dtype=torch.int64)\n",
        "    tokens_batch.append(process_text)\n",
        "    offsets.append(process_text.size(0))\n",
        "\n",
        "  labels_batch = torch.tensor(labels_batch, dtype=torch.int64)\n",
        "  offsets = torch.tensor(offsets[:-1], dtype=torch.int64).cumsum(dim=0) # Aplicar cmsum a solo filas para que indique real donde se encuentra el incio de la otra palraba ejemplo palabra 2 inicia en 5 palabra 3 inciia en 10 palabra 4 inicia en 0+5+10\n",
        "  tokens_batch = torch.cat(tokens_batch) #Ya que tenemos en una lista muchso tensores entonces lo juntamos a solo uno [todos]\n",
        "\n",
        "  return labels_batch.to(device) , tokens_batch.to(device), offsets.to(device)"
      ],
      "metadata": {
        "id": "u392gHXOP25g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EJEMPLO DE USO\n",
        "\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# train_iter = AG_NEWS(split=\"train\")\n",
        "# dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch) #collatefn es para una funcion de recopilacion es decir ejemplo [1], [2], [3] = [1, 2, 3] los junta en un solo tensor"
      ],
      "metadata": {
        "id": "MqtwU-1ZJnXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación del modelo"
      ],
      "metadata": {
        "id": "EEPTgCtp7kWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importante las librerías\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f  #Funciones creadas\n",
        "\n",
        "#Modelo\n",
        "class ModeltextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim,num_clases):\n",
        "    super(ModeltextClassifier, self).__init__()\n",
        "\n",
        "    #Primera capa procesador EMBEDDING\n",
        "    self.embed = nn.EmbeddingBag(vocab_size, embedding_dim)\n",
        "    #Capa de normalización  x - u / o para acelerar los procesos y tenga su escalas en el mismo rango y no sobreajuste\n",
        "    self.bn = nn.BatchNorm1d(embedding_dim)\n",
        "    #Capa de salida - Proyeccion lo lleva a una cantidad de clases especificas de salida\n",
        "    self.fc = nn.Linear(embedding_dim, num_clases)\n",
        "\n",
        "  def forward(self, tokens, offsets):\n",
        "    #Realizar el embedding del texto\n",
        "    embd = self.embed(tokens, offsets)\n",
        "    #Aplicar normalzacion\n",
        "    norm = self.bn(embd)\n",
        "    #Aplicar funcion de activacion\n",
        "    relu = f.relu(norm)\n",
        "    #Definir la salida (Resultado , la classe que fue )\n",
        "    return self.fc(relu)"
      ],
      "metadata": {
        "id": "b9RNCF6cY4_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este punto el embed al ser un embedingbag relamente son 4 parametors que al inico e parecen pero son diferntes\n",
        "perimero tenemos los parametros normales , de toda la vida , que se encargan de ajustar el tamaño y la cantidad de vectores a hacer\n",
        "\n",
        "Ejemplo:\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Parametros fijos:\n",
        "\n",
        "Vocab_size=20 -> 20 Vectores a hacer\n",
        "\n",
        "embeding_dim -> 10 -> Cada vector 10 espacios [1, 2, 3, 4, 5, 6, 7, 8, 9 ,10] con numeros random\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Parametros  dinamicos:\n",
        "\n",
        "Una vez puesto los otros ahora estos van a referirse a el batch es decir la divison a entrenar en ese momento\n",
        "\n",
        "data -> Cantidad total de tokens dentro del batch la cantidad de palabras totales de las soraciones concatenadas\n",
        "\n",
        "offsets -> indicará donde acaba cada frase para que el modelo pueda aplicar su embed\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Ejemplo practico:\n",
        "\n",
        "batch1:\n",
        "\n",
        "datos = ['hola, amigo' , 'adios amigo']\n",
        "tokens = ['hola', 'amigo'], ['adios']\n",
        "vocab = {'hola':1, 'amigo':2, 'adios':3}\n",
        "\n",
        "al pasarlos por el data loader y para que sea mas facil de procesar esto en la computadora todo termina siendo uno solo\n",
        "\n",
        "[tensor con todos los datos numeros -vocab -id]\n",
        "\n",
        "ya se sabe de que tamaño se hacen los embeds, pero no se sabe en donde hcer el embd entonces ya samos que hay 3 tokens pero no sabemos en donde aplicar el embed entonces entrn los offtense\n",
        "token 1 a 2 hay una oracion la otra es otra entonces ahi ya sabemos de donde a donde se aplica el embd y se aplica\n",
        "\n",
        " [[embed1], [2] hasta completar todos los embeds  \n",
        "\n",
        "\n",
        " pero normalmente este hace los embeds por cada palabra no oracion entonces con embedgin bang este aplica na operacionsea max , mean , sum y crea solo uno"
      ],
      "metadata": {
        "id": "OtceKTG7Onni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clases = len(set([label for (label, text) in train_iter]))\n",
        "model_t = ModeltextClassifier(vocab_size=len(vocab), embedding_dim=100, num_clases=clases).to(device)"
      ],
      "metadata": {
        "id": "6LiYDnZlBjxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cantida de ids\n",
        "id = len(vocab)\n",
        "print(id)\n",
        "#Arquitectura\n",
        "print(model_t)\n",
        "#Crear funcion para revisar la cantidad de parametros a ajustar\n",
        "def params_count(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Cantidad de parametros a entrenar {params_count(model_t):,}\")"
      ],
      "metadata": {
        "id": "HPDLG5N0FM8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función de entrenamiento"
      ],
      "metadata": {
        "id": "0WJXqWSlKHYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Función entrenadora del modelo\n",
        "\n",
        "def training(model, dataloader):\n",
        "  #Configurar mood entrenamiento\n",
        "  model.train()\n",
        "\n",
        "  #Definir variables para controlar el modelo, ver precisón\n",
        "  epoch_acc = 0\n",
        "  epoch_loss = 0\n",
        "  total_count = 0\n",
        "\n",
        "  #Entrenar\n",
        "  for index, (label, text, offsets) in enumerate(dataloader):\n",
        "    #Restablecer el greadiente para recalcular nueva dirección de perdida\n",
        "    optimizador.zero_grad()\n",
        "    #Realizar predicciones\n",
        "    prediccion_train = model(text, offsets)\n",
        "    #Obtener perdida\n",
        "    loss_train = criterio(prediccion_train, label)\n",
        "    #Crear el backpropagation\n",
        "    loss_train.backward()\n",
        "    #Calcular cantidad de aciertos\n",
        "    aciertos_train = (prediccion_train.argmax(1) == label).sum()#Elije la fila de indices con los valroes mas altos los quita de bool con .sum() convirtiendolos en si es true 1/0 de esa manera cada item y divide entre el dato real y da el accuracy\n",
        "    #Evitar que los gradientes se eleven mucho\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) #Recorta los parametros , que llegeun hasta 01 para que no tenga valores muy altos\n",
        "    #Realziar el re ajuste de los pesos\n",
        "    optimizador.step()\n",
        "    #realizar el accuracy\n",
        "    epoch_acc += aciertos_train.item()\n",
        "    epoch_loss += loss_train.item()\n",
        "    total_count += label.size(0)\n",
        "\n",
        "    #Revisar como vamos\n",
        "    if index % 400 == 0 and index > 0:\n",
        "        print(f'Epoch Numero: {epoch} | {index / len(dataloader)}% Batches completados | Perdida promedio: {epoch_loss / total_count} | Accuracy Promedio {epoch_acc / total_count}')\n",
        "\n",
        "  return epoch_acc / total_count , epoch_loss / total_count"
      ],
      "metadata": {
        "id": "rDWBWsanKIW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcion de evaluacion"
      ],
      "metadata": {
        "id": "DIvf85_i4N-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Crear la funcion de evaluacion\n",
        "def evaluation(modelo, dataloader):\n",
        "  #Fijar metodo de evaluacion\n",
        "  modelo.eval()\n",
        "  #Variables de informacion\n",
        "  epoch_acc_ev = 0\n",
        "  epoch_loss_ev = 0\n",
        "  total_labels_ev = 0\n",
        "\n",
        "  #El no grad desactiva el calculo de gradiente lo cual es mas eficiente para realizar procesos de evlauacion liberando espacio en memoria\n",
        "  with torch.no_grad():\n",
        "    for index, (label, text, offsets) in enumerate(dataloader):\n",
        "      #Realizar prediccion\n",
        "      prediccion_test = modelo(text, offsets)\n",
        "      #Calculo de variables informativas\n",
        "      aciertos_test = (prediccion_test.argmax(1) == label).sum()\n",
        "      #Calcular perdida del modelo\n",
        "      loss_test = criterio(prediccion_test, label)\n",
        "      #Actualizar variables locales\n",
        "      epoch_acc_ev += aciertos_test.item()\n",
        "      epoch_loss_ev += loss_test.item()\n",
        "      total_labels_ev += label.size(0)\n",
        "\n",
        "  return epoch_acc_ev / total_labels_ev, epoch_loss_ev / total_labels_ev"
      ],
      "metadata": {
        "id": "BgUhBjwVfDez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear los hyperparametros , Loss y optimizer y split"
      ],
      "metadata": {
        "id": "TMDlGhkX9bsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparams\n",
        "\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 0.1\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "Fh6y_-dq9Rr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Función de perdida\n",
        "criterio = nn.CrossEntropyLoss()\n",
        "#Optimizador\n",
        "optimizador = torch.optim.SGD(params=model_t.parameters(), lr=LEARNING_RATE, momentum=0.9)"
      ],
      "metadata": {
        "id": "9xC30nCi9-Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separación en train, val, test\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset #Transformar de iterable a map para poder usar random split para que se pueda indexar y subdibidir | dandoles ñongitud e index\n",
        "\n",
        "#Crear dataset train\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dt = to_map_style_dataset(train_iter)\n",
        "test_dt = to_map_style_dataset(test_iter)"
      ],
      "metadata": {
        "id": "ncZK7Xg3_s45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Datos para train set\n",
        "train_size = int(len(train_dt) * 0.95) #95% datos para train\n",
        "#Separación para train y test\n",
        "split_train, split_eval = random_split(train_dt, [train_size, len(train_dt) - train_size]) #Fijamos de donde saca los datos y adema´s establecemos la cantidad de datos para train y eval\n",
        "#Crear los dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "#Train dataloader\n",
        "dataloader_train = DataLoader(split_train, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "#Test dataloader\n",
        "dataloader_test = DataLoader(test_dt, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "#Val dataloader\n",
        "dataloader_val = DataLoader(split_eval, BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "WaZ7VZuSBZIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenar el modelo"
      ],
      "metadata": {
        "id": "iwjAmRBCGJNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Crear variable que devuelve la perdida de validacióon mas pequeña\n",
        "major_loss_validation = float('inf')\n",
        "\n",
        "#Entrenar\n",
        "for epoch in range(1, EPOCHS + 1 ):\n",
        "  #Entrenar el modelo\n",
        "  accuracy_mean_trian, mean_loss_train = training(model_t, dataloader_train)\n",
        "  #Validación\n",
        "  accuracy_mean_eval, mean_loss_eval = evaluation(model_t, dataloader_val)\n",
        "  #Guardar el modelo con mejores resultados\n",
        "  if mean_loss_eval < major_loss_validation:\n",
        "    major_loss_validation = mean_loss_eval\n",
        "    torch.save(model_t.state_dict(), \"mejor_model.pt\")"
      ],
      "metadata": {
        "id": "0xFRNoxrGILn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acurracy_mean_test, loss_mean_test = evaluation(model_t, dataloader_test)\n",
        "\n",
        "print(f'Promedio de accuracy para test: {acurracy_mean_test}')\n",
        "print(f'Promedio de perdida para test: {loss_mean_test}')"
      ],
      "metadata": {
        "id": "AQIZOdmUN3BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizar inferencia"
      ],
      "metadata": {
        "id": "-PkIQgyhWnnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapeo de etiquetas para hacer inferencia con datos reales texto\n",
        "labels_map = {\n",
        "    1: \"World\",\n",
        "    2: \"Sports\",\n",
        "    3: \"Business\",\n",
        "    4: \"Sci/Tech\"\n",
        "}\n",
        "\n",
        "#Crear funcón predictora\n",
        "\n",
        "def predictora(text, convertor_id):\n",
        "  with torch.inference_mode():\n",
        "    text = torch.tensor(convertor_id(text), device=device)\n",
        "    #Convencion pra modelo que usa compile para realizar esto mas rapido\n",
        "    # optim_mod = torch.compile(model_t, mode='reduce_overhead') #Busca la forma mas rapida de ejecutar el modelo para que la compitlacion no sea tardada / Maxutotune para la compitlacion mas eficiente -> mas recursos\n",
        "    salida = model_t(text, torch.tensor([0], dtype=torch.int64, device=device))#Al ser un texto el modelo va iniciar en el offset 0\n",
        "    return salida.argmax(1).item() #Del tensor mas alto, el item mas alto"
      ],
      "metadata": {
        "id": "hI5QzA_YWmkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para probar diferentes ejemplos\n",
        "labels_list = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "fk_db = [\n",
        "    [\n",
        "        \"UN Security Council meets to discuss crisis in Middle East.\",\n",
        "        \"European Union expands with new member states.\",\n",
        "        \"Russia announces major military exercise near border.\",\n",
        "        \"Thousands protest government reforms in Argentina.\",\n",
        "        \"China signs trade agreement with African nations.\"\n",
        "    ],\n",
        "    [\n",
        "        \"Lakers defeat Celtics in overtime thriller.\",\n",
        "        \"Roger Federer advances to Wimbledon final.\",\n",
        "        \"Brazil beats Argentina in Copa America clash.\",\n",
        "        \"Chicago Cubs win their first World Series in decades.\",\n",
        "        \"Michael Phelps sets new world record in swimming.\"\n",
        "    ],\n",
        "    [\n",
        "        \"Amazon reports record quarterly profits.\",\n",
        "        \"Wall Street closes higher after strong earnings reports.\",\n",
        "        \"Toyota recalls vehicles due to safety concerns.\",\n",
        "        \"Oil prices rise amid supply concerns in the Middle East.\",\n",
        "        \"Microsoft announces acquisition of gaming company.\"\n",
        "    ],\n",
        "    [\n",
        "        \"NASA announces discovery of new exoplanet.\",\n",
        "        \"Apple unveils latest iPhone model with new features.\",\n",
        "        \"Google launches AI-powered translation tool.\",\n",
        "        \"Scientists develop breakthrough cancer treatment.\",\n",
        "        \"SpaceX successfully launches new rocket.\",\n",
        "        \"Lionel Messi scored twice as Inter Miami defeated LA Galaxy 3-1 in the MLS, extending their unbeaten run to five games. The Argentine forward dazzled the crowd with a free-kick goal and an assist, proving once again why he remains one of the most influential players in world football.\"\n",
        "    ]\n",
        "]\n",
        "\n",
        "\n",
        "def probar_ejemplos(db):\n",
        "    for i, label_group in enumerate(db):\n",
        "        print('-'*200)\n",
        "        print(f'\\nSección {labels_list[i]}')\n",
        "        for text in label_group:\n",
        "            predicted_class_index = predictora(text, text_to_id)\n",
        "            predicted_class_name = labels_list[predicted_class_index]\n",
        "            print(f'Text: {text}')\n",
        "            print(f'Predicted class name: {predicted_class_name}\\n')\n",
        "\n",
        "#Ejecutando función creada\n",
        "probar_ejemplos(fk_db)"
      ],
      "metadata": {
        "id": "Ld50lVmNjgRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardado y carga de modelos\n",
        "\n",
        "el metodo state_dict() Se encarga en un diccionario de almacenar los parametros y loss sesgos del modelo para poder usarlo en otro modelo"
      ],
      "metadata": {
        "id": "sYJkWlGPE_Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Guardar modelo\n",
        "model_stat_dict = model_t.state_dict()\n",
        "#Guardar optimizador\n",
        "optimizador_state_dict = optimizador.state_dict()\n",
        "#Crear un checkpoint (Punto de guardado para luego entrenar)\n",
        "checkpoint = {\n",
        "    'model_state_dict ': model_stat_dict,\n",
        "    'optimizador_state_dict': optimizador_state_dict,\n",
        "    'epochs': EPOCHS,\n",
        "    'loss': mean_loss_train\n",
        "}\n",
        "\n",
        "#Guardarlo\n",
        "torch.save(checkpoint, 'model_saved_checkd.pth')"
      ],
      "metadata": {
        "id": "lgTjZ92KFB-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exportar a huggin face"
      ],
      "metadata": {
        "id": "6CVQ7QhUSnzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#No regrese todo lo que hace\n",
        "!pip install hugginface_hub"
      ],
      "metadata": {
        "id": "bJpbQS23Qvz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "hqWyVWjwSwf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "# api.create_repo(repo_id=\"clasification-ag_news\")"
      ],
      "metadata": {
        "id": "I92pDKuKS8F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Subir la informacion del modelo\n",
        "api.upload_file(path_or_fileobj='./model_saved_checkd.pth', path_in_repo='model_saved_checkd.pth', repo_id='JuanAcevedo/clasification-ag_news')"
      ],
      "metadata": {
        "id": "pPWRG840VQYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddfd3Ymnrbfw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}